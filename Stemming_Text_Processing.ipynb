{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Notes on the Code**\n",
        "\n",
        "### **Concept of Stemming**\n",
        "Stemming is the process of reducing words to their root or base form by removing prefixes or suffixes. This is commonly used in Natural Language Processing (NLP) to normalize text data for tasks like text classification, search engines, and sentiment analysis.\n",
        "\n",
        "Different stemming algorithms have different rules for how they reduce words to their base form. This code demonstrates three popular stemming techniques: **Porter Stemmer**, **Regexp Stemmer**, and **Snowball Stemmer**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Input Word List**\n",
        "```python\n",
        "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]\n",
        "```\n",
        "This list contains words in various forms, including:\n",
        "- **Present participle**: \"eating,\" \"writing\"\n",
        "- **Third-person singular**: \"eats,\" \"writes\"\n",
        "- **Past participle**: \"eaten,\" \"finalized\"\n",
        "- **Base form**: \"programs,\" \"history\"\n",
        "\n",
        "The goal is to reduce these words to their base forms using different stemmers.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Porter Stemmer**\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word + \"---->\" + ps.stem(word))\n",
        "```\n",
        "- **Porter Stemmer**: This is one of the oldest and most widely used stemming algorithms. It uses a set of heuristic rules to iteratively remove suffixes and reduce words to their stems.\n",
        "- Example transformations:\n",
        "  - \"eating\" → \"eat\"\n",
        "  - \"writing\" → \"write\"\n",
        "  - \"programming\" → \"program\"\n",
        "  - \"finalized\" → \"final\"\n",
        "- Additional examples:\n",
        "  - `ps.stem('congratulations')` → \"congratul\"\n",
        "  - `ps.stem('sitting')` → \"sit\"\n",
        "- **Limitations**: Porter Stemmer sometimes over-stems or under-stems, leading to results that may not always match the intended base form (e.g., \"congratulations\" → \"congratul\").\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Regexp Stemmer**\n",
        "```python\n",
        "from nltk.stem import RegexpStemmer\n",
        "reg_stem = RegexpStemmer('ing|s$|e$|able$', min=4)\n",
        "```\n",
        "- **Regexp Stemmer**: Allows custom stemming by defining a regular expression to remove specific patterns from words.\n",
        "- Parameters:\n",
        "  - `'ing|s$|e$|able$'`: Removes the suffixes \"ing,\" \"s\" (if it appears at the end), \"e,\" and \"able.\"\n",
        "  - `min=4`: Ensures that the stemmed word must be at least 4 characters long.\n",
        "- Example transformations:\n",
        "  - `reg_stem.stem('eating')` → \"eat\"\n",
        "  - `reg_stem.stem('ingeating')` → \"ingeat\"\n",
        "- **Use Case**: This stemmer is useful when you need precise control over the stemming process using custom rules.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Snowball Stemmer**\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "snow_stem = SnowballStemmer('english')\n",
        "for word in words:\n",
        "    print(word + \"----->\" + snow_stem.stem(word))\n",
        "```\n",
        "- **Snowball Stemmer**: A more advanced and efficient version of the Porter Stemmer. It is language-specific and supports multiple languages (e.g., \"english,\" \"french,\" etc.).\n",
        "- Example transformations:\n",
        "  - \"eating\" → \"eat\"\n",
        "  - \"writes\" → \"write\"\n",
        "  - \"programming\" → \"program\"\n",
        "  - \"history\" → \"histori\" (stems historical terms)\n",
        "- **Advantages**:\n",
        "  - More accurate and consistent than Porter Stemmer.\n",
        "  - Better handling of irregular suffixes and edge cases.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Stemmers**\n",
        "| **Stemmer**       | **Description**                                                                                     | **Use Case**                                 |\n",
        "|--------------------|----------------------------------------------------------------------------------------------------|---------------------------------------------|\n",
        "| **Porter Stemmer** | Heuristic-based, iterative stemming process.                                                       | Quick and basic stemming.                   |\n",
        "| **Regexp Stemmer** | Customizable stemming using regular expressions.                                                   | Fine-tuned stemming for specific patterns.  |\n",
        "| **Snowball Stemmer** | Advanced and efficient stemming with better language-specific handling than Porter Stemmer.       | More precise and language-aware stemming.   |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code demonstrates how to use three different stemming algorithms to reduce words to their base form. Depending on the context and requirements, you can choose:\n",
        "- **Porter Stemmer** for quick and basic stemming.\n",
        "- **Regexp Stemmer** for precise control over suffix removal.\n",
        "- **Snowball Stemmer** for more accurate and language-specific stemming.\n",
        "\n",
        "By normalizing words using these techniques, NLP tasks like text mining, classification, and search indexing can be improved."
      ],
      "metadata": {
        "id": "b7EEjmKB4NPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTjzttMG7kCw"
      },
      "outputs": [],
      "source": [
        "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##PorterStremmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "lGNR648h85Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+ ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fynCH8Rt9LAW",
        "outputId": "1bdc7417-3047-4cbb-a1bb-dbe5466de61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tsEpFX_v9khn",
        "outputId": "f3624df5-6975-4c56-edcf-ae792ba76c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Zynhp8z39ssL",
        "outputId": "943a1f65-f403-4cc4-b86a-80bce40e44df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##RegexpStemmer\n",
        "from nltk.stem import RegexpStemmer\n",
        "reg_stem = RegexpStemmer('ing|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "7F_guo5v-BSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qX0G4Ye--ZAA",
        "outputId": "7e6f2246-9a88-4c4b-eb86-784cfe16fa87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yHNu4eyi-oep",
        "outputId": "1a75f077-d110-4b29-e55b-3e4428fc5553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SnowBall Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "snow_stem = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "QWoG_-MG-9oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"----->\"+snow_stem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "few4v5TA_VYc",
        "outputId": "9691829e-6271-406b-ca6b-ed8cf68f8b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----->eat\n",
            "eats----->eat\n",
            "eaten----->eaten\n",
            "writing----->write\n",
            "writes----->write\n",
            "programming----->program\n",
            "programs----->program\n",
            "history----->histori\n",
            "finally----->final\n",
            "finalized----->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQ2ZI1f7_t0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}