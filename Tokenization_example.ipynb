{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Notes on the Code**\n",
        "\n",
        "### **Concept of Tokenization**\n",
        "Tokenization is the process of breaking down a given text into smaller components, such as sentences or words. It is a fundamental step in Natural Language Processing (NLP) that helps in text analysis and processing. There are two main types of tokenization demonstrated in this code:\n",
        "1. **Sentence Tokenization**: Splits a paragraph or large text into individual sentences.\n",
        "2. **Word Tokenization**: Splits sentences or paragraphs into individual words.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Install NLTK Library**\n",
        "The line `!pip install nltk` installs the **Natural Language Toolkit (NLTK)**, a library widely used for NLP tasks.\n",
        "\n",
        "#### **2. Input Corpus**\n",
        "```python\n",
        "corpus = \"\"\"Hello Welcome, My name is Armghan.\n",
        "I'm currently learning NLP to effecrively land a Job in Gen Ai field.\n",
        "Hopefully i'll get it in this year.\n",
        "\"\"\"\n",
        "```\n",
        "The `corpus` variable contains a sample text that will be tokenized into sentences and words.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Sentence Tokenization**\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "documents = sent_tokenize(corpus)\n",
        "```\n",
        "- **`sent_tokenize()`**: Breaks the input text into sentences.\n",
        "- **`nltk.download('punkt')`**: Downloads the `punkt` tokenizer model, which is necessary for sentence tokenization.\n",
        "- **Output**: The variable `documents` will store a list of sentences:\n",
        "  ```python\n",
        "  ['Hello Welcome, My name is Armghan.',\n",
        "   \"I'm currently learning NLP to effecrively land a Job in Gen Ai field.\",\n",
        "   \"Hopefully i'll get it in this year.\"]\n",
        "  ```\n",
        "\n",
        "#### **4. Printing Sentences**\n",
        "```python\n",
        "for sentence in documents:\n",
        "    print(sentence)\n",
        "```\n",
        "This loop iterates through the `documents` list and prints each sentence.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Word Tokenization**\n",
        "##### **Paragraph to Words**\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)\n",
        "```\n",
        "- **`word_tokenize()`**: Splits the entire paragraph into words (including punctuation).\n",
        "- **Output**:\n",
        "  ```python\n",
        "  ['Hello', 'Welcome', ',', 'My', 'name', 'is', 'Armghan', '.',\n",
        "   'I', \"'m\", 'currently', 'learning', 'NLP', 'to', 'effecrively',\n",
        "   'land', 'a', 'Job', 'in', 'Gen', 'Ai', 'field', '.',\n",
        "   'Hopefully', 'i', \"'ll\", 'get', 'it', 'in', 'this', 'year', '.']\n",
        "  ```\n",
        "\n",
        "##### **Sentence to Words**\n",
        "```python\n",
        "for sentence in documents:\n",
        "    print(word_tokenize(sentence))\n",
        "```\n",
        "This loop applies `word_tokenize()` to each sentence from the `documents` list and prints the tokenized words for each sentence separately.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. WordPunct Tokenizer**\n",
        "```python\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)\n",
        "```\n",
        "- **`wordpunct_tokenize()`**: Splits the text into words, treating punctuation as separate tokens.\n",
        "- **Output**:\n",
        "  ```python\n",
        "  ['Hello', 'Welcome', ',', 'My', 'name', 'is', 'Armghan', '.',\n",
        "   'I', \"'\", 'm', 'currently', 'learning', 'NLP', 'to',\n",
        "   'effecrively', 'land', 'a', 'Job', 'in', 'Gen', 'Ai',\n",
        "   'field', '.', 'Hopefully', 'i', \"'\", 'll', 'get',\n",
        "   'it', 'in', 'this', 'year', '.']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Treebank Word Tokenizer**\n",
        "```python\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)\n",
        "```\n",
        "- **`TreebankWordTokenizer()`**: A tokenizer based on the Penn Treebank conventions. It handles splitting contractions (e.g., \"I'm\" â†’ [\"I\", \"'m\"]) and punctuation.\n",
        "- **Output**:\n",
        "  ```python\n",
        "  ['Hello', 'Welcome', ',', 'My', 'name', 'is', 'Armghan', '.',\n",
        "   'I', \"'m\", 'currently', 'learning', 'NLP', 'to', 'effecrively',\n",
        "   'land', 'a', 'Job', 'in', 'Gen', 'Ai', 'field', '.',\n",
        "   'Hopefully', 'i', \"'ll\", 'get', 'it', 'in', 'this', 'year', '.']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "- **Sentence Tokenization**: Splits a text into sentences using `sent_tokenize()`.\n",
        "- **Word Tokenization**:\n",
        "  - Splits text into words using `word_tokenize()` and other tokenizers.\n",
        "  - Handles punctuation as separate tokens (e.g., `wordpunct_tokenize()`).\n",
        "  - Treebank tokenizer uses specific rules to handle contractions and punctuation.\n",
        "\n",
        "Each tokenizer has its own purpose and use case. Experimenting with these helps to understand which tokenizer is best suited for specific NLP tasks."
      ],
      "metadata": {
        "id": "oDzsFlYY3ZUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNWEE5tleVYA",
        "outputId": "834cbde9-9570-4287-c2a3-8e29544def2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome, My name is Armghan.\n",
        "I'm currently learning NLP to effecrively land a Job in Gen Ai field.\n",
        "Hopefully i'll get it in this year.\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "AcvJNYTN3MPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JirbI3ir35XW",
        "outputId": "4df23d36-1f61-4e58-ef83-fb66518d414e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, My name is Armghan.\n",
            "I'm currently learning NLP to effecrively land a Job in Gen Ai field.\n",
            "Hopefully i'll get it in this year.\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenization\n",
        "## Sentence-->paragraphs.\n",
        "import nltk # Import the nltk module\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnq9ZOq44C7-",
        "outputId": "2c343c6c-1623-4f9b-c243-bb2001ce1bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "GFppMhl94neM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUmpaAjB5EW2",
        "outputId": "bc584a9f-ec44-450f-e787-19a7379c3990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MoeKT9M5H8s",
        "outputId": "cda0e053-aa09-48cf-f1cc-dea8bb251630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, My name is Armghan.\n",
            "I'm currently learning NLP to effecrively land a Job in Gen Ai field!.\n",
            "Hopefully i'll get it in this year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenization\n",
        "##Paragrah--> words\n",
        "##Sentence--> words\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "hIwsuJqz5VpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1cpJHhj5jjS",
        "outputId": "0a848b3a-af40-41ba-aeb2-9a7c140ee2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Armghan',\n",
              " '.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'currently',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'to',\n",
              " 'effecrively',\n",
              " 'land',\n",
              " 'a',\n",
              " 'Job',\n",
              " 'in',\n",
              " 'Gen',\n",
              " 'Ai',\n",
              " 'field',\n",
              " '.',\n",
              " 'Hopefully',\n",
              " 'i',\n",
              " \"'ll\",\n",
              " 'get',\n",
              " 'it',\n",
              " 'in',\n",
              " 'this',\n",
              " 'year',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVLJNPOZ56dk",
        "outputId": "60ecd43c-849a-444b-e494-4c97537b7d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'My', 'name', 'is', 'Armghan', '.']\n",
            "['I', \"'m\", 'currently', 'learning', 'NLP', 'to', 'effecrively', 'land', 'a', 'Job', 'in', 'Gen', 'Ai', 'field', '.']\n",
            "['Hopefully', 'i', \"'ll\", 'get', 'it', 'in', 'this', 'year', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "3GA24Jkm6LWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGmRwrTg6Snn",
        "outputId": "98330eb5-a3a9-4655-e630-db671924d41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Armghan',\n",
              " '.',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'currently',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'to',\n",
              " 'effecrively',\n",
              " 'land',\n",
              " 'a',\n",
              " 'Job',\n",
              " 'in',\n",
              " 'Gen',\n",
              " 'Ai',\n",
              " 'field',\n",
              " '.',\n",
              " 'Hopefully',\n",
              " 'i',\n",
              " \"'\",\n",
              " 'll',\n",
              " 'get',\n",
              " 'it',\n",
              " 'in',\n",
              " 'this',\n",
              " 'year',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tree Bank word tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n"
      ],
      "metadata": {
        "id": "5oLLuz-d6e0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "x8l7F9VG6otL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtEzbk8z6shS",
        "outputId": "682272e8-d4f7-4142-c028-e282987f402d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Armghan.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'currently',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'to',\n",
              " 'effecrively',\n",
              " 'land',\n",
              " 'a',\n",
              " 'Job',\n",
              " 'in',\n",
              " 'Gen',\n",
              " 'Ai',\n",
              " 'field.',\n",
              " 'Hopefully',\n",
              " 'i',\n",
              " \"'ll\",\n",
              " 'get',\n",
              " 'it',\n",
              " 'in',\n",
              " 'this',\n",
              " 'year',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}