{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### **1. Text Preprocessing**\n",
        "- **Sentences (`sent`)**: These are sample text sentences used as input.\n",
        "- **Vocabulary Size (`voc_size`)**: This defines the range of integer values used for encoding words. In real applications, it's usually based on the dataset size.\n",
        "- **One-Hot Encoding**: Converts words into unique integers within the range `[0, voc_size-1]` using Keras's `one_hot()` method. This step is a prerequisite for feeding data into the Embedding layer.\n",
        "\n",
        "#### **2. Sequence Padding**\n",
        "- **Why Padding?** Sentences have varying lengths, but deep learning models require inputs of uniform shape. Padding ensures all sequences are the same length by adding zeros (or truncating) to reach a fixed `sent_length`.\n",
        "- **Pre-padding**: Zeros are added at the beginning of sequences (`padding='pre'`).\n",
        "\n",
        "#### **3. Word Embeddings**\n",
        "- **Embedding Layer**:\n",
        "  - Maps each word (integer) to a dense vector of fixed size (`output_dim` or `dim`).\n",
        "  - The embedding layer is initialized randomly and learns meaningful representations during training.\n",
        "  - Input shape is inferred automatically when padding is used, so `input_length` is unnecessary.\n",
        "\n",
        "#### **4. Model Compilation**\n",
        "- **Optimizer (`adam`)**: A popular optimization algorithm that adapts learning rates during training.\n",
        "- **Loss Function (`mse`)**: Placeholder for demonstration. For text tasks, a categorical or sparse cross-entropy loss is typically used.\n",
        "\n",
        "#### **5. Model Summary**\n",
        "- Displays the structure of the model, including layers, output shapes, and trainable parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Concepts\n",
        "1. **Word Embeddings**:\n",
        "   - Dense vector representations of words that capture semantic relationships.\n",
        "   - Words with similar meanings or contexts have closer embeddings.\n",
        "\n",
        "2. **One-Hot Encoding vs. Word Embeddings**:\n",
        "   - One-hot encoding creates sparse, high-dimensional vectors where only one element is `1`.\n",
        "   - Embeddings create dense, low-dimensional vectors that are learned during training.\n",
        "\n",
        "3. **Padding**:\n",
        "   - Uniform sequence lengths are critical for batch processing in deep learning.\n",
        "\n",
        "4. **Why Use an Embedding Layer?**\n",
        "   - Avoids manual feature engineering by learning data-specific embeddings.\n",
        "   - Reduces input dimensionality and improves model efficiency.\n",
        "\n",
        "This code serves as a foundation for working with text data, enabling you to preprocess input, pad sequences, and create embeddings using Keras."
      ],
      "metadata": {
        "id": "_uyarhvLBbSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mCFk9JR12WfF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentences that we have\n",
        "# List of sentences (sample dataset)\n",
        "\n",
        "sent=[  'the glass of milk',\n",
        "     'the glass of juice',\n",
        "     'the cup of tea',\n",
        "    'I am a good boy',\n",
        "     'I am a good developer',\n",
        "     'understand the meaning of words',\n",
        "     'your videos are good',]\n"
      ],
      "metadata": {
        "id": "-UlUuCLt3En0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFofR8LB3gWV",
        "outputId": "a56490b1-9621-4862-8d4c-0f3edcaa8258"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the glass of milk',\n",
              " 'the glass of juice',\n",
              " 'the cup of tea',\n",
              " 'I am a good boy',\n",
              " 'I am a good developer',\n",
              " 'understand the meaning of words',\n",
              " 'your videos are good']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size for one-hot encoding (arbitrarily chosen; typically depends on dataset)\n",
        "voc_size=500"
      ],
      "metadata": {
        "id": "Y6MGq17f3gVj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate one-hot encoded representation for each sentence\n",
        "# Each word in a sentence is mapped to an integer between 0 and voc_size - 1\n",
        "onehot_repr=[one_hot(words,voc_size)for words in sent]\n",
        "print(onehot_repr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzYFaHYX3pg3",
        "outputId": "d217ba3f-d043-41b7-c7b8-198ee72386b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[313, 201, 92, 134], [313, 201, 92, 244], [313, 316, 92, 261], [239, 84, 360, 140, 324], [239, 84, 360, 140, 425], [381, 313, 201, 92, 256], [6, 330, 109, 140]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Embedding Representation\n",
        "from tensorflow.keras.layers import Embedding, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "p1m0uU033pf-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PdF_2-f746Jy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-padding\n",
        "# Set a fixed sequence length for all sentences (padding or truncating as necessary)\n",
        "sent_length=8\n",
        "# Pad the one-hot encoded sentences with zeros (pre-padding) to ensure uniform length\n",
        "# This is necessary for input into the Embedding layer, as it expects uniform input dimensions\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQpdVI-e49zD",
        "outputId": "12915576-538b-41f0-9d37-f6baad6b3365"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0 313 201  92 134]\n",
            " [  0   0   0   0 313 201  92 244]\n",
            " [  0   0   0   0 313 316  92 261]\n",
            " [  0   0   0 239  84 360 140 324]\n",
            " [  0   0   0 239  84 360 140 425]\n",
            " [  0   0   0 381 313 201  92 256]\n",
            " [  0   0   0   0   6 330 109 140]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dimensionality of word embeddings\n",
        "dim=10 #Feature representation dimension if you have larger dataset use somewhere around 300 or 300"
      ],
      "metadata": {
        "id": "BaOduxkD6VL3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model using Input layer\n",
        "# Create a Sequential model with an Embedding layer\n",
        "# Add an Embedding layer\n",
        "# - input_dim: Vocabulary size (voc_size), the number of unique integers in the input data\n",
        "# - output_dim: Embedding dimension (dim), the size of the dense vector representing each word\n",
        "# - The Embedding layer learns the embeddings during training\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(sent_length,)),  # Specify input shape\n",
        "    Embedding(input_dim=voc_size, output_dim=dim)\n",
        "])\n",
        "# Compile the model with the Adam optimizer and Mean Squared Error (MSE) loss\n",
        "# - The loss function and optimizer settings here are placeholders, as this model is not yet trained\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# Display a summary of the model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "Jz3sj7fs6uu9",
        "outputId": "ee67a7f1-3cd7-44fc-d964-5b9ab157c7e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │           \u001b[38;5;34m5,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,000\u001b[0m (19.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,000</span> (19.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,000\u001b[0m (19.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,000</span> (19.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qothB1lD7G3C",
        "outputId": "8cf907ef-8810-48f7-ba73-75651f5aff7c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0, 313, 201,  92, 134], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(embedded_docs[0].reshape(1, -1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVcAlJvp9v72",
        "outputId": "c88c45b6-1ce9-4c8f-f4a7-ec0eb2ad258e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV1eRw8d-DkZ",
        "outputId": "ef084bc0-0c18-4dad-f269-63e71a991072"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.04429581 -0.03727648  0.04626607 -0.00178454  0.00846018\n",
            "    0.02377937  0.02640684  0.01891888 -0.00699757 -0.0066157 ]\n",
            "  [-0.04591066 -0.03623826 -0.02711327 -0.02671381 -0.04603697\n",
            "    0.04737509  0.03400474 -0.0309536   0.03483728  0.00633885]\n",
            "  [-0.04657155  0.03075459 -0.00354611  0.03127669 -0.04636631\n",
            "    0.0389827  -0.04626621 -0.02154769  0.0328099  -0.02025727]\n",
            "  [ 0.00858914  0.00389517 -0.04662341  0.02347447  0.03368037\n",
            "   -0.04104428 -0.00757492  0.02280011 -0.00926981 -0.01381532]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(embedded_docs.reshape(1, -1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a5vtySd-LGt",
        "outputId": "32d8ae81-a666-4b08-bb29-efdcbcef4626"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "[[[ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.04429581 -0.03727648  0.04626607 -0.00178454  0.00846018\n",
            "    0.02377937  0.02640684  0.01891888 -0.00699757 -0.0066157 ]\n",
            "  [-0.04591066 -0.03623826 -0.02711327 -0.02671381 -0.04603697\n",
            "    0.04737509  0.03400474 -0.0309536   0.03483728  0.00633885]\n",
            "  [-0.04657155  0.03075459 -0.00354611  0.03127669 -0.04636631\n",
            "    0.0389827  -0.04626621 -0.02154769  0.0328099  -0.02025727]\n",
            "  [ 0.00858914  0.00389517 -0.04662341  0.02347447  0.03368037\n",
            "   -0.04104428 -0.00757492  0.02280011 -0.00926981 -0.01381532]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.04429581 -0.03727648  0.04626607 -0.00178454  0.00846018\n",
            "    0.02377937  0.02640684  0.01891888 -0.00699757 -0.0066157 ]\n",
            "  [-0.04591066 -0.03623826 -0.02711327 -0.02671381 -0.04603697\n",
            "    0.04737509  0.03400474 -0.0309536   0.03483728  0.00633885]\n",
            "  [-0.04657155  0.03075459 -0.00354611  0.03127669 -0.04636631\n",
            "    0.0389827  -0.04626621 -0.02154769  0.0328099  -0.02025727]\n",
            "  [ 0.00646736  0.0146428   0.01674564  0.04089331 -0.00827228\n",
            "   -0.0180793  -0.02783072 -0.01971581 -0.02219437  0.04655408]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.04429581 -0.03727648  0.04626607 -0.00178454  0.00846018\n",
            "    0.02377937  0.02640684  0.01891888 -0.00699757 -0.0066157 ]\n",
            "  [-0.03439567  0.00385057 -0.04206052 -0.04657597  0.0297013\n",
            "    0.02881766 -0.03899731 -0.04729899 -0.04975928 -0.04038358]\n",
            "  [-0.04657155  0.03075459 -0.00354611  0.03127669 -0.04636631\n",
            "    0.0389827  -0.04626621 -0.02154769  0.0328099  -0.02025727]\n",
            "  [-0.04205508 -0.04232804 -0.01500751 -0.0358787  -0.01352657\n",
            "    0.02771205  0.02725729  0.00884247  0.02031041  0.03836051]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.03212703 -0.02510536  0.02682671  0.00399852 -0.01853782\n",
            "   -0.00844835  0.04186502  0.03924194  0.01022221 -0.01306138]\n",
            "  [-0.00405646 -0.03409425 -0.01469525  0.00178285 -0.03183559\n",
            "    0.03000302  0.04673417  0.03597932 -0.04220894 -0.02464732]\n",
            "  [ 0.00390904  0.00072256  0.02261234 -0.03975786  0.01897328\n",
            "    0.0122064  -0.013292   -0.02440473 -0.00961211  0.03441792]\n",
            "  [-0.03553291 -0.02653682 -0.01191714  0.01577432  0.01934013\n",
            "   -0.03478911 -0.04547409  0.0069049   0.01717398 -0.01215725]\n",
            "  [ 0.03295151  0.00703983  0.02471128 -0.04571682 -0.04179287\n",
            "    0.0016898  -0.02837463  0.00550831 -0.02166054  0.00607787]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.03212703 -0.02510536  0.02682671  0.00399852 -0.01853782\n",
            "   -0.00844835  0.04186502  0.03924194  0.01022221 -0.01306138]\n",
            "  [-0.00405646 -0.03409425 -0.01469525  0.00178285 -0.03183559\n",
            "    0.03000302  0.04673417  0.03597932 -0.04220894 -0.02464732]\n",
            "  [ 0.00390904  0.00072256  0.02261234 -0.03975786  0.01897328\n",
            "    0.0122064  -0.013292   -0.02440473 -0.00961211  0.03441792]\n",
            "  [-0.03553291 -0.02653682 -0.01191714  0.01577432  0.01934013\n",
            "   -0.03478911 -0.04547409  0.0069049   0.01717398 -0.01215725]\n",
            "  [-0.01823167  0.00425462 -0.02472571 -0.00754289 -0.02218833\n",
            "    0.04146521 -0.00717426  0.04895839  0.04191807 -0.00265165]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.02081994  0.03423895  0.02884812 -0.0168482   0.00277004\n",
            "    0.01811827  0.01491705  0.01946486  0.04419836 -0.00213252]\n",
            "  [-0.04429581 -0.03727648  0.04626607 -0.00178454  0.00846018\n",
            "    0.02377937  0.02640684  0.01891888 -0.00699757 -0.0066157 ]\n",
            "  [-0.04591066 -0.03623826 -0.02711327 -0.02671381 -0.04603697\n",
            "    0.04737509  0.03400474 -0.0309536   0.03483728  0.00633885]\n",
            "  [-0.04657155  0.03075459 -0.00354611  0.03127669 -0.04636631\n",
            "    0.0389827  -0.04626621 -0.02154769  0.0328099  -0.02025727]\n",
            "  [ 0.01220725  0.01537647  0.03227197  0.01439471 -0.0383\n",
            "    0.01740475 -0.00783529 -0.02869254 -0.0364213  -0.0088528 ]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [ 0.02551533 -0.01755876  0.00032029 -0.04363328 -0.02342428\n",
            "   -0.0191457  -0.01017138 -0.00359224 -0.00796907 -0.01994984]\n",
            "  [-0.04226112  0.00180783  0.0220522   0.0458158   0.03287907\n",
            "   -0.00768445  0.01107117 -0.02150676 -0.01401041  0.01540582]\n",
            "  [ 0.00414776 -0.00326494 -0.02741597  0.03019246  0.03009636\n",
            "    0.02873014  0.00263401 -0.03499181  0.03907439  0.04599008]\n",
            "  [ 0.02841682  0.04370082 -0.02527558  0.03505811  0.0346537\n",
            "   -0.04880785  0.00311267 -0.04845316  0.03891489  0.03640508]\n",
            "  [-0.03553291 -0.02653682 -0.01191714  0.01577432  0.01934013\n",
            "   -0.03478911 -0.04547409  0.0069049   0.01717398 -0.01215725]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jvkXJyY-xwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}