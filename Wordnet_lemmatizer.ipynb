{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Notes on the Code**\n",
        "\n",
        "### **Concept of Lemmatization**\n",
        "Lemmatization is the process of reducing a word to its **base or dictionary form** (called a \"lemma\") while considering the word's **part of speech (POS)**. Unlike stemming, which often truncates words without understanding their meaning, lemmatization produces meaningful base forms that exist in the dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "```\n",
        "- **`WordNetLemmatizer`**: A lemmatizer provided by NLTK that uses the WordNet lexical database to find the base form of words.\n",
        "- **`nltk.download('wordnet')`**: Downloads the WordNet corpus, which is necessary for the lemmatizer to function.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Lemmatization with POS**\n",
        "```python\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "'''\n",
        "POS- Noun-n\n",
        "POS- Verb-v\n",
        "POS- Adjective-a\n",
        "POS- Adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\", pos='v')\n",
        "```\n",
        "- **Part of Speech (POS)**: Lemmatization relies on specifying the word's grammatical role. The supported POS tags are:\n",
        "  - **'n'**: Noun\n",
        "  - **'v'**: Verb\n",
        "  - **'a'**: Adjective\n",
        "  - **'r'**: Adverb\n",
        "- Example:\n",
        "  - `\"going\"` with `pos='v'` → \"go\" (base verb form).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Lemmatizing a List of Words**\n",
        "```python\n",
        "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]\n",
        "for word in words:\n",
        "    print(word + \"----->\" + lemmatizer.lemmatize(word, pos='v'))\n",
        "```\n",
        "- Here, each word in the list is lemmatized using the **verb ('v')** POS tag.\n",
        "- Example transformations:\n",
        "  - `\"eating\"` → \"eat\"\n",
        "  - `\"writing\"` → \"write\"\n",
        "  - `\"programming\"` → \"program\"\n",
        "  - `\"finalized\"` → \"finalize\"\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Lemmatization Without Specifying POS**\n",
        "```python\n",
        "lemmatizer.lemmatize(\"goes\")\n",
        "```\n",
        "- If the POS is not specified, the lemmatizer assumes the word is a **noun ('n')**.\n",
        "- Example:\n",
        "  - `\"goes\"` (default POS: noun) → \"go\" (still correct here).\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Lemmatizing Adverbs and Other POS**\n",
        "```python\n",
        "lemmatizer.lemmatize(\"fairly\", pos='v'), lemmatizer.lemmatize(\"sportingly\", pos='v')\n",
        "```\n",
        "- Lemmatizing adverbs or less common POS forms may not always yield significant changes.\n",
        "- Examples:\n",
        "  - `\"fairly\"` with `pos='v'` → \"fairly\" (no change).\n",
        "  - `\"sportingly\"` with `pos='v'` → \"sportingly\" (no change).\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison: Stemming vs. Lemmatization**\n",
        "| **Feature**        | **Stemming**                               | **Lemmatization**                          |\n",
        "|---------------------|--------------------------------------------|--------------------------------------------|\n",
        "| **Approach**        | Rule-based truncation.                    | Dictionary-based normalization.            |\n",
        "| **POS Consideration** | Does not consider POS.                  | Requires specifying POS for accuracy.      |\n",
        "| **Output**          | May produce non-meaningful stems.         | Produces meaningful base words (lemmas).   |\n",
        "| **Accuracy**        | Less accurate.                           | More accurate, context-aware.              |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "1. Lemmatization provides **contextually meaningful base forms**, unlike stemming, which may produce truncated or non-meaningful words.\n",
        "2. Specifying the correct **POS tag** enhances lemmatization accuracy.\n",
        "3. Use lemmatization when you need **semantically valid base forms** for tasks like information retrieval, machine translation, and linguistic analysis.\n",
        "\n",
        "---\n",
        "\n",
        "This code demonstrates how the **WordNetLemmatizer** can effectively normalize text, considering part-of-speech tags for precise processing."
      ],
      "metadata": {
        "id": "liFTJ0wX5Jtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qEFfnDVbU3j",
        "outputId": "0e500066-ba1d-474d-94a3-4c431faff894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "'''\n",
        "POS- Noun-n\n",
        "POS- Verb-v\n",
        "POS- Adjective-a\n",
        "POS- Adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RJdM69opgGgS",
        "outputId": "57132e64-928b-4768-9aab-264caf820fa2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "5pgoKHlUgoPU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"----->\"+lemmatizer.lemmatize(word, pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOCscDraguzZ",
        "outputId": "cbba33eb-f40c-4dcc-a117-61e53cd23a12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----->eat\n",
            "eats----->eat\n",
            "eaten----->eat\n",
            "writing----->write\n",
            "writes----->write\n",
            "programming----->program\n",
            "programs----->program\n",
            "history----->history\n",
            "finally----->finally\n",
            "finalized----->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"goes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rqRGuYJohNd6",
        "outputId": "db31a78b-5dc5-4fdb-880e-84548bc37d87"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dey-_BbThRHr",
        "outputId": "cfab92b8-87df-4914-a2a3-27ec42d9d6db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}