{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Notes on the Code**\n",
        "\n",
        "### **Purpose of the Code**\n",
        "This script demonstrates the application of:\n",
        "1. **Stopword Removal**: Filters out commonly used words (e.g., \"is,\" \"and,\" \"the\") that do not add significant meaning to text analysis.\n",
        "2. **Stemming**: Reduces words to their root forms using the **PorterStemmer** and **SnowballStemmer**.\n",
        "3. **Lemmatization**: Normalizes words to their dictionary forms using the **WordNetLemmatizer**.\n",
        "4. The code processes a given paragraph by tokenizing it into sentences, removing stopwords, and applying either stemming or lemmatization to the words.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Setup**\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punk_tab')\n",
        "nltk.download('punkt_tab')\n",
        "stopwords.words('english')\n",
        "```\n",
        "- **Imports Required Libraries**: Includes tools for stemming, lemmatization, tokenization, and stopword removal.\n",
        "- **Downloads Required NLTK Data**:\n",
        "  - `stopwords`: List of common words to filter out.\n",
        "  - `punkt_tab`: Tokenization rules.\n",
        "- **`stopwords.words('english')`**: Loads the stopwords list for English.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Tokenize Paragraph into Sentences**\n",
        "```python\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "type(sentences)\n",
        "```\n",
        "- **`nltk.sent_tokenize`**: Splits the paragraph into sentences for processing.\n",
        "- **`type(sentences)`**: Confirms the result is a list of sentences.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Stopword Removal and PorterStemmer Stemming**\n",
        "```python\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "```\n",
        "- **PorterStemmer**:\n",
        "  - Reduces words to their root forms (e.g., \"running\" → \"run\").\n",
        "  - It applies a set of rules to strip suffixes like \"-ing,\" \"-ed,\" or \"-s.\"\n",
        "- **Stopword Filtering**:\n",
        "  - Excludes words that do not contribute significant meaning, such as \"the,\" \"is,\" and \"and.\"\n",
        "- **Tokenization**:\n",
        "  - Splits each sentence into words for processing using `nltk.word_tokenize`.\n",
        "- **Rejoin Words**:\n",
        "  - After processing, the words are reassembled into sentences using `' '.join()`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Stopword Removal and SnowballStemmer Stemming**\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "Snowstemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [Snowstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "```\n",
        "- **SnowballStemmer**:\n",
        "  - Another stemming algorithm, optimized for multiple languages.\n",
        "  - Typically more aggressive than PorterStemmer.\n",
        "- **Processing Logic**:\n",
        "  - Similar to PorterStemmer processing but uses SnowballStemmer for word normalization.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Stopword Removal and WordNetLemmatizer**\n",
        "```python\n",
        "Lemma = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [Lemma.lemmatize(word, pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "```\n",
        "- **WordNetLemmatizer**:\n",
        "  - Converts words to their base dictionary forms (lemmas) while considering their part of speech (POS).\n",
        "  - **POS Tag**:\n",
        "    - Verb (`'v'`) is specified to enhance the accuracy of lemmatization.\n",
        "  - Examples:\n",
        "    - `\"running\"` → `\"run\"`\n",
        "    - `\"written\"` → `\"write\"`\n",
        "- **Process**:\n",
        "  - Identical to the stemming logic but applies lemmatization for more semantically accurate results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "\n",
        "1. **Stopwords Removal**:\n",
        "   - Filters out common words that provide minimal information.\n",
        "   - Reduces noise in text data.\n",
        "\n",
        "2. **PorterStemmer vs. SnowballStemmer**:\n",
        "   - Both aim to reduce words to their roots.\n",
        "   - **PorterStemmer**: Simpler, rule-based.\n",
        "   - **SnowballStemmer**: More advanced and language-flexible.\n",
        "\n",
        "3. **Lemmatization**:\n",
        "   - Produces valid dictionary words, unlike stemming, which may produce truncated results.\n",
        "   - Context-aware: Considers part-of-speech (POS) for better accuracy.\n",
        "\n",
        "4. **Comparison Example**:\n",
        "   - **Word**: \"running\"\n",
        "     - **PorterStemmer**: \"run\"\n",
        "     - **SnowballStemmer**: \"run\"\n",
        "     - **Lemmatizer**: \"run\" (if POS is `'v'`)\n",
        "\n",
        "This script showcases the process of refining raw text for downstream tasks like text analysis, classification, or information retrieval."
      ],
      "metadata": {
        "id": "4g_xQ8Wn58K8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JjwJ5uZ0h5N0"
      },
      "outputs": [],
      "source": [
        "paragraph = \"\"\"\n",
        "The increasing reliance on technology has transformed nearly every aspect of modern life, from how we communicate to how we work and learn.\n",
        " Smartphones, laptops, and other digital devices have become indispensable tools, enabling people to stay connected and access vast amounts of information at their fingertips. Despite the undeniable benefits, this technological shift has brought its own set of challenges.\n",
        "  People often find themselves overwhelmed by the constant flow of notifications, emails, and messages, making it difficult to focus on meaningful tasks.\n",
        "   Furthermore, the rise of social media has led to debates about its impact on mental health, as individuals are exposed to unrealistic standards and excessive comparisons.\n",
        "    While technology offers immense opportunities for innovation and progress, it is equally important to address the societal and personal implications it carries to ensure a balanced and sustainable future.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punk_tab')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wKljdW7i2m5",
        "outputId": "4886bcf9-e028-4d43-f8fb-892cd17b70cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading punk_tab: Package 'punk_tab' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcbQ5-JXjPP0",
        "outputId": "cef28d52-2000-4cd7-8f5b-4ee4766e6b38"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "RvVKTh65jzJQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =  nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "oXUOmCm4kExI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-qGhnXJkedU",
        "outputId": "d61902f8-1c7d-401a-cd7e-639da170b4ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwards and filter and then Apply Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words) # converting all list of the words into sentences\n",
        ""
      ],
      "metadata": {
        "id": "9RyExYo_kg-s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9dWE75zlg5t",
        "outputId": "a3c87679-eefc-4de5-b125-73e0aefd6390"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the increas relianc technolog transform nearli everi aspect modern life , commun work learn .',\n",
              " 'smartphon , laptop , digit devic becom indispens tool , enabl peopl stay connect access vast amount inform fingertip .',\n",
              " 'despit undeni benefit , technolog shift brought set challeng .',\n",
              " 'peopl often find overwhelm constant flow notif , email , messag , make difficult focu meaning task .',\n",
              " 'furthermor , rise social media led debat impact mental health , individu expos unrealist standard excess comparison .',\n",
              " 'while technolog offer immens opportun innov progress , equal import address societ person implic carri ensur balanc sustain futur .']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "Snowstemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "6ogjfNyglsYH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwards and filter and then Apply Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [Snowstemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words) # converting all list of the words into sentences\n",
        ""
      ],
      "metadata": {
        "id": "Xw32K7malxr-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go2u_CXUlxrF",
        "outputId": "1d53fe8e-520c-4d63-fa4d-cf84759deb42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the increas relianc technolog transform near everi aspect modern life , communic work learn .',\n",
              " 'smartphon , laptop , digit devic becom indispens tool , enabl peopl stay connect access vast amount inform fingertip .',\n",
              " 'despit undeni benefit , technolog shift brought set challeng .',\n",
              " 'peopl often find overwhelm constant flow notif , email , messag , make difficult focus meaning task .',\n",
              " 'furthermor , rise social media led debat impact mental health , individu expos unrealist standard excess comparison .',\n",
              " 'while technolog offer immens opportun innov progress , equal import address societ person implic carri ensur balanc sustain futur .']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jBWjfFYdmhVr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwards and filter and then Apply Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [Lemma.lemmatize(word,pos='v')for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words) # converting all list of the words into sentences\n",
        ""
      ],
      "metadata": {
        "id": "uaz5v4MnmpBT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc65JN6fmpAZ",
        "outputId": "9a50d7a6-ed03-4f33-9856-9c3cc4e87218"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The increase reliance technology transform nearly every aspect modern life , communicate work learn .',\n",
              " 'Smartphones , laptops , digital devices become indispensable tool , enable people stay connect access vast amount information fingertips .',\n",
              " 'Despite undeniable benefit , technological shift bring set challenge .',\n",
              " 'People often find overwhelm constant flow notifications , email , message , make difficult focus meaningful task .',\n",
              " 'Furthermore , rise social media lead debate impact mental health , individuals expose unrealistic standards excessive comparisons .',\n",
              " 'While technology offer immense opportunities innovation progress , equally important address societal personal implications carry ensure balance sustainable future .']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}